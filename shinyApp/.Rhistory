setwd("~/workspaceR")
setRepositories
setRepositories(ind = c(1:6, 8))
source("https://bioconductor.org/biocLite.R")
biocLite()
install.packages("edgeRun")
require(knitr)
require(stringi)
library(data.table)
# The https method does not work in all machines and so I have replaced it with http. It is only necessary to download this file once. So we check to see if the file already exists, and in that case we skip the download .
blogs <-readLines("./final/en_US/en_US.blogs.txt", encoding = "UTF-8", skipNul=TRUE)
news <- readLines("./final/en_US/en_US.news.txt", encoding = "UTF-8", skipNul=TRUE)
twitter <- readLines("./final/en_US/en_US.twitter.txt", encoding = "UTF-8", skipNul=TRUE)
textSource = c(blogs, news, twitter)
format(object.size(textSource), units="GB")
#save(textSource, file = "./backupData/textsource.RData")
filteredWordsFile = "./data/badwords.txt"
wordsToRemove = read.table(filteredWordsFile, stringsAsFactors = FALSE)
profanityList = as.character(wordsToRemove[, 1])
set.seed(2015)
############### Atencion
p = 0.80
###############
numSampleLines = round(length(textSource) * p, 0)
sampledLines = sort(sample(1:length(textSource), numSampleLines, replace = FALSE))
textData = textSource[sampledLines]
#save("textData", file = "./data/textData.RData")
nonSampledLines = (1:length(textSource))[-sampledLines]
p2 = 0.50
numHeldOutLines = round(length(nonSampledLines) * p2, 0)
heldOutLines = sort(sample(nonSampledLines, numHeldOutLines, replace = FALSE))
sum(heldOutLines %in% sampledLines)
testLines = (1:length(textSource))[-c(sampledLines, heldOutLines)]
sum(testLines %in% heldOutLines)
sum(testLines %in% sampledLines)
sum(sapply(list(sampledLines, heldOutLines, testLines), length))
length(textSource)
heldOutText = textSource[heldOutLines]
#saveRDS("heldOutText", file = "./data/heldOutText.RDS")
testText = textSource[testLines]
#saveRDS("testText", file = "./data/testText.RDS")
rm(twitter)
rm(blogs)
rm(news)
rm(textSource)
#############################################################################
#############################################################################
#############################################################################
## Processing sampled text lines
#############################################################################
#############################################################################
#############################################################################
sentences = stri_split_boundaries(textData, type="sentence")
sentences = unlist(sentences)
names(sentences) = NULL
head(sentences, 40)
rm(textData)
sentences = tolower(sentences)
sentences = paste(" BS ", sentences)
names(sentences) = NULL
head(sentences, 20)
#############################################################################
## From sentences to words
#############################################################################
listWords = stri_extract_all_words(sentences)
listWords = unlist(listWords)
head(listWords, 200)
rm(sentences)
#############################################################################
## Detecting years, numbers
#############################################################################
regexYEAR = "^[12]{1}[0-9]{3}$"
listWords = stri_replace_all(listWords, replacement = "YEAR", regex = regexYEAR)
regexNUM = "^[[:digit:]]+(,[[:digit:]]*)*\\.*[[:digit:]]*$"
listWords = stri_replace_all(listWords, replacement = "NUM", regex = regexNUM)
names(listWords) = NULL
head(listWords, 2000)
#############################################################################
## Removing punctuation
#############################################################################
punctuationRegex = "[\\[\\]\\$\\*\\+\\.\\?\\^\\{\\}\\|\\(\\)\\#%&~_/<=>✬!,:;❵@]"
listWords = stri_replace_all(listWords, replacement = "", regex = punctuationRegex)
listWords = stri_replace_all(listWords, replacement = "'", fixed = "’")
#############################################################################
## Processing non-English characters
#############################################################################
nonEnglishChars = "[^A-Za-z'-[:space:]]"
listWords = stri_replace_all(listWords, replacement = "", regex = nonEnglishChars)
#############################################################################
## Removing empty words
#############################################################################
emptyWords = (listWords == "")|(listWords=="'")
listWords = listWords[!emptyWords]
rm(emptyWords)
#############################################################################
#############################################################################
#############################################################################
## Creating a data table with the preprocessed data
#############################################################################
#############################################################################
#############################################################################
wordsDT = data.table(x = listWords)
wordsDT
rm(listWords)
#############################################################################
## Adding the counts
#############################################################################
wordsDT[ , cX:=.N, by=x]
wordsDT
format(object.size(wordsDT), units="Mb")
#############################################################################
## Creating a dictionary of numerical codes for words
#############################################################################
codes = copy(wordsDT)
nrow(codes)
codes = unique(codes)
nrow(codes)
setkey(codes, cX)
codes[ , code:= nrow(codes):1]
codes
setkey(codes, x)
setwd("~/workspaceR/project_guia/CourseraJHUDataScienceCapstone")
require(knitr)
require(stringi)
library(data.table)
# The https method does not work in all machines and so I have replaced it with http. It is only necessary to download this file once. So we check to see if the file already exists, and in that case we skip the download .
blogs <-readLines("./final/en_US/en_US.blogs.txt", encoding = "UTF-8", skipNul=TRUE)
news <- readLines("./final/en_US/en_US.news.txt", encoding = "UTF-8", skipNul=TRUE)
twitter <- readLines("./final/en_US/en_US.twitter.txt", encoding = "UTF-8", skipNul=TRUE)
textSource = c(blogs, news, twitter)
format(object.size(textSource), units="GB")
#save(textSource, file = "./backupData/textsource.RData")
filteredWordsFile = "./data/badwords.txt"
wordsToRemove = read.table(filteredWordsFile, stringsAsFactors = FALSE)
profanityList = as.character(wordsToRemove[, 1])
set.seed(2015)
############### Atencion
p = 0.80
###############
numSampleLines = round(length(textSource) * p, 0)
sampledLines = sort(sample(1:length(textSource), numSampleLines, replace = FALSE))
textData = textSource[sampledLines]
#save("textData", file = "./data/textData.RData")
nonSampledLines = (1:length(textSource))[-sampledLines]
p2 = 0.50
numHeldOutLines = round(length(nonSampledLines) * p2, 0)
heldOutLines = sort(sample(nonSampledLines, numHeldOutLines, replace = FALSE))
sum(heldOutLines %in% sampledLines)
testLines = (1:length(textSource))[-c(sampledLines, heldOutLines)]
sum(testLines %in% heldOutLines)
sum(testLines %in% sampledLines)
sum(sapply(list(sampledLines, heldOutLines, testLines), length))
length(textSource)
heldOutText = textSource[heldOutLines]
#saveRDS("heldOutText", file = "./data/heldOutText.RDS")
testText = textSource[testLines]
#saveRDS("testText", file = "./data/testText.RDS")
rm(twitter)
rm(blogs)
rm(news)
rm(textSource)
#############################################################################
#############################################################################
#############################################################################
## Processing sampled text lines
#############################################################################
#############################################################################
#############################################################################
sentences = stri_split_boundaries(textData, type="sentence")
sentences = unlist(sentences)
names(sentences) = NULL
head(sentences, 40)
rm(textData)
sentences = tolower(sentences)
sentences = paste(" BS ", sentences)
names(sentences) = NULL
head(sentences, 20)
#############################################################################
## From sentences to words
#############################################################################
listWords = stri_extract_all_words(sentences)
listWords = unlist(listWords)
head(listWords, 200)
rm(sentences)
#############################################################################
## Detecting years, numbers
#############################################################################
regexYEAR = "^[12]{1}[0-9]{3}$"
listWords = stri_replace_all(listWords, replacement = "YEAR", regex = regexYEAR)
regexNUM = "^[[:digit:]]+(,[[:digit:]]*)*\\.*[[:digit:]]*$"
listWords = stri_replace_all(listWords, replacement = "NUM", regex = regexNUM)
names(listWords) = NULL
head(listWords, 2000)
#############################################################################
## Removing punctuation
#############################################################################
punctuationRegex = "[\\[\\]\\$\\*\\+\\.\\?\\^\\{\\}\\|\\(\\)\\#%&~_/<=>✬!,:;❵@]"
listWords = stri_replace_all(listWords, replacement = "", regex = punctuationRegex)
listWords = stri_replace_all(listWords, replacement = "'", fixed = "’")
#############################################################################
## Processing non-English characters
#############################################################################
nonEnglishChars = "[^A-Za-z'-[:space:]]"
listWords = stri_replace_all(listWords, replacement = "", regex = nonEnglishChars)
#############################################################################
## Removing empty words
#############################################################################
emptyWords = (listWords == "")|(listWords=="'")
listWords = listWords[!emptyWords]
rm(emptyWords)
#############################################################################
#############################################################################
#############################################################################
## Creating a data table with the preprocessed data
#############################################################################
#############################################################################
#############################################################################
wordsDT = data.table(x = listWords)
wordsDT
rm(listWords)
#############################################################################
## Adding the counts
#############################################################################
wordsDT[ , cX:=.N, by=x]
wordsDT
format(object.size(wordsDT), units="Mb")
#############################################################################
## Creating a dictionary of numerical codes for words
#############################################################################
codes = copy(wordsDT)
nrow(codes)
codes = unique(codes)
nrow(codes)
setkey(codes, cX)
codes[ , code:= nrow(codes):1]
codes
setkey(codes, x)
lowFreq = 2
format(object.size(codes), units="Mb")
codes2 = copy(codes[cX > lowFreq])
format(object.size(codes2), units="Mb")
saveRDS(codes2, file = "./data/codes.Rds")
codes2Remove = copy(wordsToRemove)
nrow(codes2Remove)
codes2Remove = unique(codes2Remove)
nrow(codes2Remove)
setkey(codes2Remove, cX)
codes2Remove[ , code:= nrow(codes2Remove):1]
codes2Remove
setkey(codes2Remove, x)
wordsDTRemove = data.table(x = wordsToRemove$V1)
wordsDTRemove
wordsDTRemove[ , cX:=.N, by=x]
format(object.size(wordsDTRemove), units="Mb")
codes2Remove = copy(wordsDTRemove)
nrow(codes2Remove)
codes2Remove = unique(codes2Remove)
nrow(codes2Remove)
setkey(codes2Remove, cX)
codes2Remove[ , code:= nrow(codes2Remove):1]
codes2Remove
setkey(codes2Remove, x)
format(object.size(codes2Remove), units="Mb")
codes2Remove2 = copy(codes2Remove[cX > lowFreq])
format(object.size(codes2Remove2), units="Mb")
saveRDS(codes2Remove2, file = "./data/codes2Remove.Rds")
setwd("~/workspaceR/DataScienceSpecializationFinalProject/shinyApp")
unigrams = readRDS("../data/unigrams.Rds") #ngrams$unigrams
unigrams = readRDS("./data/unigrams.Rds") #ngrams$unigrams
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
